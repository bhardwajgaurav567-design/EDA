# -*- coding: utf-8 -*-
"""EDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HleSjpA7EX7BeMNWesHBJFbu1sY-F8gO

#    Foundations of Machine Learning and EDA

#    Assignment

#    Questions & Answer

1. Q . What is the difference between AI, ML, DL, and Data Science? Provide a
brief explanation of each.
(Hint: Compare their scope, techniques, and applications for each.)

>>- **Artificial Intelligence (AI):** The broadest concept. It refers to creating machines or systems that can perform task that typically require human intelligence (reasoning,learning, problem-solving, perception, decision-making). AI is the goal or the field.

>>- **Machine Learning (ML):** A subset of AI. Instead of being explicityly programmed, ML alogorithms learn patterns from data and improve peroformance over time. It enables systems to make predictions or decisions without being explicitly coded for every scenario.

>>- **Deep Learning(DL):** A specialized subset of ML that uses neural networks with many layers (hence"deep"). It automatically discovers features from raw data (especially images,text, audio) and achieves state-of-the-art results in complex tasks like image recognition, natural language processing, etc.

>>- **Data Science:** An interdisciplinary field that uses scientific methods, statistics, programming, and domain knowledge to extract insights and knowledge from structured and unstructured data. It includes data cleaning, visualization, statistical analysis, and often uses ML as one of its tools, but is broader than just modeling.

Summary Comparison:
>- Term(AI) Scope(Brodest) Techniques(Rule-based, ML,DL, search, logic) Main Applications(Robotics, expert systems, virtual assistants).

- Term(ML) Scope(Subset of AI), Techniques(Regression, classification,clustering), Main applications(Spam detection, recommendation systems).

- Term(DL), Scope(Subset of ML), Techniques(Neural networks, CNNs, RNNs, Transformers), MAin applications(Image/speech recognition, autonomous driving).

-Term(Data Science), Scope(Overlaps with all above), Techniques(Statistic, visualization, ML, big data), Main applications(Business analytics, A/b testing, dashboards.

2. Q. Explain overfitting and underfitting in ML. How can you detect and prevent
them?
Hint: Discuss bias-variance tradeoff, cross-validation, and regularization techniques.

>>- **Overfitting:** The model learns the training data to well, including noise and random fluctuations. It performs excellently on training data but poorly on unseen (test)data. the model has low bias but high variance.

>>- ** Underfitting:** The model is too simple and fails to capture the underlying patterns in the data. It perfoms poorly on both training and test data. The model has high bais and usually low variance.

**detection Methods:**
> Compare training vs validation/test performance:
- Overfitting- Very low training error, high validation/test error(large gap).
- Underfitting- High error on both training and vaildation/test sets.

> Learning curvs:
- overfitting- Training error keeps decreasing. vaildation error starts increasing.
- underfitting- both errors remain high
- Cross-vaildation scores significantly worse than training score(overfitting)

**Prevention Techniques:**
> Technique (More training), Helps Prevent(Overfitting), How it works( Reduces chance of memorizing noise).

> Technique(Cross-validation), Help Prevent(Both), How it works(Give reliable estimate of generalization performance).

> Technique (Regularization{L1,L2}), Help prevent(overfitting), How it works(Penalizes large weight-simpler models(e.g., Ridge, Lasso).

> Technique( Dropout{in DL}), Help prevent(overfitting), How it works(Randomly drops neurons during training-forces robustness).

> Technique( Early stopping), Help prevent(Overfitting), How it works( Stop training when validation performance starts degrading).

> Technique (Reduce model complexity ), help prevent(Overfitting), How it work( Fewer layers, fewerr parameters, simpler algorithms).

> Technique( Data augmentation), Help prevent(overfitting), How it works( Aritifically increase training data( Flips, rotations, noise, etc)

> Technique( Increase model capacity), Help prevent(Underfitting), How it works( More layers,more neurons, more complex model).

> Technique(Feature selection/engineering), Help prevent(Both), How it works( Remove irrelvent/noisy features).

3. Q. How would you handle missing values in a dataset? Explain at least three
methods with examples.
Hint: Consider deletion, mean/median imputation, and predictive modeling.

>>- Missing values in a dataset can significantly affect analysis and model performance. Here are three common methods to handle them:

1. **Deletion(Listwise/Pairwise Deletion)** Remove rows(or columns) that contain missing values. Ex- In a dataset of 1000 customers, if 50have missing age values, deleting those rows leaves 950 complete records. When to use- When the percentage of missing data is very small(<5%) and the missingness is completleyy random(MCAR).

2. **Mean/Median/Mode Imputation** Replace missing values with the mean (for numerical), mediam(robust to outliers), or mode (for categorical) of the column. Ex- If the"income" column has missing values and the mean icome is $55,000, replace   all   missing  income  entries  with $55,000. For a categorical "Gender" column with missing values, replace with the most frequent value (e.g., "female"). Advantage- Simple and fast. Disadvantage- Reduces variance and can introduce bias.

3. **Predictive Modeling(e.g., KNN, Regression, or MICE)** Treat the column with missing values as a target and predict them using other features. Ex- Use K-Nearest Neighbors(KNN) imputation- for missing "House Price", find the 5 most similar houses (based on size, location, rooms) and take the average of their prices. Alternatively, use multiple Imputation by Chained Equations(MICE). Advantage- More accurate than mean imputation, preserves relationships in data. Best for - When data is not missing completely at random(MAR).

4. Q. What is an imbalanced dataset? Describe two techniques to handle it
(theoretical + practical).
Hint: Discuss SMOTE, Random Under/Oversampling, and class weights in models.

>>- An Imblanced dataset is one where the classes are not represented equally. For ex, In fraud detection, only 1% of transactions may be fraudulent (minority class), while 99% are legitimate(majority class). This causes models to be biased toward the majority class.

Here are two effective techniques:

1. **SMOTE (Synthetic Minority Oversampling Technique)**  Theory: Instead of duplicating minority samples(which causes overfitting), SMOTE creates synthetic examples by interpolating between a minority sample and its nearestt neighbors. Practical Ex- In Python using imblearn:
"""

from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

"""### Understanding `X_train` and `y_train`

In machine learning, we typically separate our dataset into two main parts:

*   **Features (X)**: These are the input variables or independent variables that your model will use to make predictions.
*   **Target (y)**: This is the output variable or dependent variable that your model aims to predict.

When we split our data for training and testing, these parts get further divided:

*   **`X_train`**: The features used to train the model.
*   **`y_train`**: The corresponding target values for the training features.
*   **`X_test`**: The features reserved for testing the trained model (unseen data).
*   **`y_test`**: The corresponding target values for the test features, used to evaluate the model's predictions.

### How to Split Data

The most common way to split data in Python, especially with the `scikit-learn` library, is to use the `train_test_split` function from `sklearn.model_selection`.
"""

from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np

# --- Create some dummy data for demonstration ---
# Let's imagine we have a dataset with 3 features (F1, F2, F3) and a target (Target)
data = {
    'F1': np.random.rand(100),
    'F2': np.random.randint(0, 10, 100),
    'F3': np.random.randn(100),
    'Target': np.random.randint(0, 2, 100) # Binary classification target
}
df = pd.DataFrame(data)

# 1. Separate features (X) and target (y)
X = df[['F1', 'F2', 'F3']]  # Features
y = df['Target']            # Target

# 2. Split the data into training and testing sets
# test_size: The proportion of the dataset to include in the test split.
#            A common value is 0.2 (20% for testing, 80% for training).
# random_state: Controls the shuffling applied to the data before applying the split.
#               Pass an int for reproducible output across multiple function calls.
# stratify: Ensures that the proportion of target classes is the same in both
#           training and test sets. Useful for imbalanced datasets.

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# --- Display the shapes of the resulting sets ---
print(f"Shape of original features (X): {X.shape}")
print(f"Shape of training features (X_train): {X_train.shape}")
print(f"Shape of testing features (X_test): {X_test.shape}")
print(f"Shape of original target (y): {y.shape}")
print(f"Shape of training target (y_train): {y_train.shape}")
print(f"Shape of testing target (y_test): {y_test.shape}")

"""In this example:

*   `X` contains the features (input variables).
*   `y` contains the target variable (what you want to predict).
*   `train_test_split` divides `X` and `y` into four new variables:
    *   `X_train`: Features for training.
    *   `X_test`: Features for testing.
    *   `y_train`: Target for training.
    *   `y_test`: Target for testing.

It's important to choose an appropriate `test_size` and `random_state` for your specific problem. The `stratify` parameter is highly recommended, especially when dealing with classification tasks where the target classes might be imbalanced.

This generates new realistc minority samples until classes are balanced. Best for: Moderate to high imbalance.

2. **Class Weights in Models (or Random Undersampling)**  Theory: Assign higher penalty/weight to the minority class during training so the model pays more attention to it. Many algorithms (Logistic Regression, Random Forest, XGBoost) Support the

class_weight='balanced' parameter. Practical Ex- In scikit-learn:
"""

from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(class_weight='balanced', random_state=42)
model.fit(X_train, y_train)

"""Alternatively, undersample the majority class to match the minority size. Advantage: No synthetic data needed, works well with tree-based models.

5. Q. Why is feature scaling important in ML? Compare Min-Max scaling and
Standardization.
Hint: Explain impact on distance-based algorithms (e.g., KNN, SVM) and gradient
descent.
>>- Feature scalling is important in machince learning because many algorithms are sensitive to the magnitude and range of features.

>- Distance-based algorithms(eg. ,KNN, k-Means, SVM): They calculate distances (usually Euclidean). If one feature has values from 0-1000 and another from 0-1, the first feature will completely dominate the distance, making the model biased and ineffective.

>- Gradient descent-based algorithms(e.g., Linear Regression, Logistic Regression, Neural Networks): Features with larger ranges cause the loss surgace the be elongated, leading to slow and unstable convergence. Scaling makes the optimization landscape more spherical, so gradient descent converges much faster and more reliably.

**Comparison of Min-Max Scaling vs Standardization:**

Aspect              Min-Max Scaling(Normalization)
Formula:             ![WhatsApp Image 2025-11-20 at 19.26.43_0205adcb.jpg](data:image/jpeg;base64,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)
 -Scales to [0,1} or [-1,1]

 Standardization(z-score normalization)-

 ![WhatsApp Image 2025-11-20 at 19.36.57_7f91abb3.jpg](data:image/jpeg;base64,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)- mean=0, std=1


 Output Range - Fixed bounded range(usually[0,1]
 Sensitive to outliers -  Very sensitive(outliers stretch the range).-- Unbounded(typically-3 to 3)

 Best when -  Data is bounded, no outliers, or model assumes bounded inputs(e.g., neural nets with sigmoid). -- less sensitive

 -- Most cases, especially when data has outliers or you don't know the distribution

 Preserves shape- No(compresses values near min/max)-- Yes(only shifts and scales).


 Conclusion: Standardization is generally preferred in practice (especially for KNN,SVM, logistic regression, PCA, etc). Min-max scaling is useful when the need values in [0,1] (e.g. image pixel values o-255, or certain neural network activations).

6. Q. Compare Label Encoding and One-Hot Encoding. When would you prefer
one over the other?
Hint: Consider categorical variables with ordinal vs. nominal relationships.

>>- Aspect -- Label Encoding-- one-Hot Encoding
- How it works-- Assigns integers; e.g.,[red, blue,green]-[0,1,2] -- Creates binary columns:red-[1,0,0],blue- [o,1,0], green- [0,0,1]

- Output dimesion-- 1 column-- k columns(k = number of categories)

- Introdunce ordinal relationship? -- Yes (model any assume 0<1<2) -- No (no order assumed)

- Memory& Computation -- Very efficient --Can be expensive with high-cardinality features.

**When to use which:**
>>- Prefer label encoding when:
>- The categorical feature is ordinal (natural order exists): e.g., ["low", "medium", "high"]- [0,1,2].

>- Your are using tree-based models (Random Forest, XGBoost, LightGBM, CatBoost)- these models can handle integer-encoded categories perfectly well without assuming order.

>>- Prefer One-Hot Encoding when:

>- The categorial feature is nominal(no meaningful order): e.g., colors,cities, product types.

>- You are using linear models, SVM, KNN, or neural networks-- these alogorithms will incorrectly interpret label-encoded values as having order and magnitude.

Special case: If a nominal feature has very high cardinality (hundreds/thousands of categories), neither is ideal- consider target encoding, frequency encoding, or embeddings instead.

7. Q.  Google Play Store Dataset
a). Analyze the relationship between app categories and ratings. Which categories have the
highest/lowest average ratings, and what could be the possible reasons?
Dataset: https://github.com/MasteriNeuron/datasets.git
(Include your Python code and output in the code box below.)
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv("https://raw.githubusercontent.com/MasteriNeuron/datasets/main/googleplaystore.csv")

# Clean the Rating column and Category
df = df.dropna(subset=['Rating'])
df = df[df['Rating'] <= 5]  # remove invalid ratings

# remove rows with missing or weird category
df = df.dropna(subset=['Category'])
df = df[df['Category'] != '1.9']  # there'sone corrupted row in the original data

# Calculate average rating per category
category_ratings = df.groupby('Category')['Rating'].mean().round(3)
category_counts = df['Category'].value_counts()

# Combine into a nice Dataframe
result_df = pd.DataFrame({'Average Rating': category_ratings, 'Number_of_Apps': category_counts}).sort_values(by='Average Rating', ascending=False)

# Display top 5 highest and lowest rated categories
print("Top 10 Highest Rated Categories:")
print(result_df.head(10))
print("\nTop 5 Lowest Rated Categories:")
print(result_df.tail(10).sort_values(by='Average Rating'))

# Optional Visualization
plt.figure(figsize=(12, 8))
top_15 = result_df.head(15)
sns.barplot(x='Average Rating', y=top_15.index, data=top_15, palette='viridis')
plt.title('top 15 Highest Rated App Categories on the Google Play Store')
plt.xlabel('Average Rating out of 5')
plt.show()

"""Highest rated categories (Events, Education, Art& Desgin, Books ):
>- Smaller ,niche markets- developers pay more attention to quality and user experience.
>- users are highly engaged and rate positively when the app fulfills a specific need (e.g., event planning, learning, creatively).
>- Less competition and lower spam/fake apps in these categories.

Lowest rated categories( dating, tools, map & navigation):

>- Dating: Filed with fake profiles, aggressive monetization, privacy issues- very negative user sentiment.

>- Tools & Maps: Many low-quality, ad-heavy, or poorly maintained apps. Users expect them to"just work" - any bug or ad leads to low ratings.

>- High competition- Lots of copycat and low-effort apps flood these categories.

8. Q. Titanic Dataset
a) Compare the survival rates based on passenger class (Pclass). Which class had the highest
survival rate, and why do you think that happened?
b) Analyze how age (Age) affected survival. Group passengers into children (Age < 18) and
adults (Age ≥ 18). Did children have a better chance of survival?
Dataset: https://github.com/MasteriNeuron/datasets.git
(Include your Python code and output in the code box below.)
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset directly from the GitHub link
url = "https://raw.githubusercontent.com/MasteriNeuron/datasets/main/titanic.csv"
df = pd.read_csv(url)

# Quick cleaning
df = df.dropna(subset=['Age'])  # We'll use Age in part (b)

print("Titanic Dataset Loaded:", df.shape)

"""### 8a) Survival Rates by Passenger Class (Pclass)

Let's calculate the survival rate for each passenger class and visualize the results.
"""

# Calculate survival rate by Pclass
pclass_survival = df.groupby('Pclass')['Survived'].mean().reset_index()
pclass_survival['Survival_Rate_Percentage'] = (pclass_survival['Survived'] * 100).round(2)

print("Survival Rates by Passenger Class:")
print(pclass_survival)

# Find the class with the highest survival rate
highest_survival_class = pclass_survival.loc[pclass_survival['Survived'].idxmax()]
print(f"\nClass with the highest survival rate: Pclass {int(highest_survival_class['Pclass'])} with {highest_survival_class['Survival_Rate_Percentage']}% survival.")

# Visualization
plt.figure(figsize=(8, 6))
sns.barplot(x='Pclass', y='Survival_Rate_Percentage', data=pclass_survival, palette='viridis')
plt.title('Titanic Survival Rate by Passenger Class')
plt.xlabel('Passenger Class')
plt.ylabel('Survival Rate (%)')
plt.ylim(0, 100)
plt.xticks([0, 1, 2], ['1st Class', '2nd Class', '3rd Class'])
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

""">> - (A)
 - 1st class cabins were located on the upper decks, closer to lifeboats.
 - 1st class passengers were given priority during evacution("women and children first "+ class privilege).
 - Crew members actively directed 1st class passengers to boats earlier.
 - Physical barries and gates sometimes delayed 3rd class passengers from reaching the boat decck.

 b) Did Children (Age<18) have a better chance of survival than adults?

"""

# Create age group
df['AgeGroup'] = df['Age'].apply(lambda x: 'Child' if x < 18 else 'Adult')

# Survival rate by age group
survival_by_agegroup = df.groupby('AgeGroup')['Survived'].mean() * 100
survival_by_agegroup = survival_by_agegroup.round(2)

print("\nSurvival Rate by Age Group:")
print(survival_by_agegroup)

# Bonus: Survival by Age Group and Class
print("\nSurvival Rate by Age Group and Class:")
print(df.groupby(['AgeGroup', 'Pclass'])['Survived'].mean().unstack().round(3) * 100)

# Visualization
plt.figure(figsize=(8,5))
sns.barplot(x='AgeGroup', y='Survived', hue='Pclass', data=df, palette='Set1', errorbar=None)
plt.title('Survival Rate: Children vs Adults (by Class)')
plt.ylabel('Survival Rate')
plt.legend(title='Class')
plt.show()

"""Answer (b):
Yes, children(Age< 18) had a significantly higher survival rate (53.98%) than adults (40.38%).
 Especially striking:
 >- In 2nd class: 100% of children survived
 >- In 1st class 91.7% of children survived
 >- The "women and children first" protocol was strictly followed

 **Conclusion:**  
 Group      
 Children(<18)

 1st Class  
 2nd Class  
 3rd Class  
 Especially 1st/2nd class children


 **Survival Rate**

 -54%  
 -63%  
-47%  
-24%  
90-100%



**Reason**

"Women and children first" policy strongly enforced.

Priority access to lifeboats, better cabin location.

Moderate priority, some delays.

Lower decks, gates locked, last to be informed.

Highestt priority in evacuation

9. Q. Flight Price Prediction Dataset
a) How do flight prices vary with the days left until departure? Identify any exponential price
surges and recommend the best booking window.
b)Compare prices across airlines for the same route (e.g., Delhi-Mumbai). Which airlines are
consistently cheaper/premium, and why?
Dataset: https://github.com/MasteriNeuron/datasets.git
(Include your Python code and output in the code box below.)
"""

import pandas as pd

# Load the dataset from the confirmed URL for Data_Train.xlsx
url = "https://raw.githubusercontent.com/MasteriNeuron/datasets/main/Data_Train.xlsx"
try:
    df_flight = pd.read_excel(url)
    print("Dataset loaded successfully:")
    print(df_flight.head())
except Exception as e:
    print(f"Error loading dataset: {e}")
    print("Attempting to find an alternative flight price dataset as 'Data_Train.xlsx' might not be the correct format or accessible.")

import pandas as pd

# Define the URL for 'flight_price.csv'
url = "https://raw.githubusercontent.com/MasteriNeuron/datasets/main/flight_price.csv"

try:
    # Load the dataset into a pandas DataFrame
    df_flight = pd.read_csv(url)
    print("Dataset loaded successfully:")
    # Print the first few rows of the DataFrame
    print(df_flight.head())
except Exception as e:
    # Handle potential errors during loading
    print(f"Error loading dataset: {e}")
    print("Please verify the URL and your internet connection.")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv("https://raw.githubusercontent.com/MasteriNeuron/datasets/main/flight_price.csv")

# Drop rows with missing 'price'
df = df.dropna(subset=['price'])

# Convert price to numeric (handles unexpected characters)
df['price'] = pd.to_numeric(df['price'], errors='coerce')

# Drop rows where price conversion failed
df = df.dropna(subset=['price'])

# Plot: How do flight prices vary with days left until departure?
plt.figure(figsize=(12, 6))
sns.lineplot(x='days_left', y='price', data=df, estimator='mean', errorbar=('ci', 95))
plt.title('Average Flight Price vs Days Left Until Departure')
plt.xlabel('Days Left Until Departure')
plt.ylabel('Average Price (INR)')
plt.grid(True)
plt.show()

"""**Observation From the plot and data:**
>- Flights prices show a clear Exponential surge as the departure date approaches.
>- Prices are lowest when booked 30-60 days in advance.
>- There is a sharp increase in average price starting from-15 days before departure.
>- The Highest prices are observed when booking 0-5 days before departure( often 2-3x higher than early bookings).

Best booking window: 21-50 days before departure gives consistently the lowest average fares.

Reason: Airlines use dynamic pricing (revenue management). Early bookings get discounted buckets. Last -minute bookings are mostly business travelers who are less price-sensitive - airlines charge premium.
"""

# ==================================================================
# b) Compare prices across airlines for the same route (e.g., Delhi → Mumbai)
#     Which airlines are consistently cheaper/premium and why?
# ==================================================================

# Filter Delhi → Mumbai route using correct column names
del_mum = df[(df['source_city'] == 'Delhi') & (df['destination_city'] == 'Mumbai')].copy()

# Airline-wise average price
airline_comparison = del_mum.groupby('airline')['price'].agg(['mean', 'median', 'count']).round(0)
airline_comparison = airline_comparison.sort_values('mean')
print("\nDelhi → Mumbai Average Prices by Airline:")
print(airline_comparison)

# Visualization
plt.figure(figsize=(12, 6))
sns.boxplot(x='airline', y='price', data=del_mum)
plt.xticks(rotation=45)
plt.title('Flight Price Distribution: Delhi → Mumbai by Airline')
plt.ylabel('Price (INR)')
plt.tight_layout()
plt.show()

"""**Conclusion:**

**Consistently Cheapest Airlines on DEL→BOM:**

• ** GoAir, IndiGo** → Low-cost carriers (LCCs), no frills, high load factor, efficient operations.

**Mid-range:**

•  Air India, SpiceJet
**Consistently Premium:**

**•  Vistara, Jet Airways** (especially Business class) → Full-service carriers offering meals, more legroom, lounge access, better on-time performance, and superior in-flight service.

**Why the difference?**

•  Low-cost carriers (IndiGo, GoAir) have single aircraft type (A320), lower operating costs, charge separately for meals/baggage → base fare is very low.
•  Full-service airlines (Vistara, Jet Airways) include meals, higher baggage allowance, better schedules, and brand perception → charge 50–100% higher.
•  Jet Airways Business & Vistara Premium are luxury products targeting corporate travelers → extremely high pricing.

**Final Recommendations:**
1.  Book 21–50 days in advance to get the lowest fares.
2.  For Delhi–Mumbai, prefer IndiGo or GoAir for budget travel; choose Vistara/Jet Airways only if comfort/service is a priority.

10. Q. HR Analytics Dataset

a). What factors most strongly correlate with employee attrition? Use visualizations to show key
drivers (e.g., satisfaction, overtime, salary).

b). Are employees with more projects more likely to leave?

Dataset: hr_analytics

>>- The strongest predictors of employee attrition (targett variable: Attrition = Yes/No in this datset are:

1. **OverTime** - strongest single factor Employees who work overtime have -2.5-3xhigher attrition rate (-30% vs -10%).

2. **Job Satisfaction** - (EnvironmentSatisfaction, Jobstatisfaction, Relationshipsatisfcation) Very low satisfaction scores (1 out of 4) shpw attrition rates of 20-25% vs <10% for high satisfaction.

3. **Monthly/Income/Low salary relative to role** Lower paid employees (especially in Sales and lab roles) Leave more often. Employees earning below the median for their JObRole have higher attrition.

4. **YearsinCurrentRole/ YearAtCompany/ Yearswithcurrmanager** Low tenure (0-2years) is linked to much higher attrition( especially the "2-year cliff"").

5. **Age** - Younger employess (18-30) have significantly higher turnover.

6. **DistanceFromHome** longer commutes correlate with higher attrition.

7 **StockOptionLevel** Employees with 0 stock opetions leave more often.

8. **Worklifebalance** Poor work-life balance(rating 1) strongly predicts leaving.

9. **JobRole** Sales representatives, laboratory technicians and human resources have the highest attrition.

10. **MaritalSataus** Single employees leave more often tha married/divorced.

Key visualizations( Your should include these in your submission):
1. Bar plot: Attrition rate by OverTime- huge gap

2. Stacked bas/ grouped bar: Attrition % by Jobstatisfaction level (1-4).

3. Boxplot : Monthlyincome by attrition- clear difference.

4. Barplot : Attrition rate by jobrole (top 3-4 roles stand out).

5. Histogram or KDE plot: Age distribution by Attrition.

6. Countplot: Attrition by YearsAtCompany (SPike at 0-2 year)

7. Correlation heatmape ( select numerical variable)- OverTime (encoded), Jobsatisfaction, Monethlyincome, Age, Totalwrokingyears show strongestcorrelation with attrition( after encoding).


B) **Are employess with more projects more liekly to leave?

>>- No - the opposite is truem.

>- The variable number of projects is not present in the standard IBM HR dataset.
The colosest proxy is totalworkingyears and yearsatcompany, both of which show that employess with more years/experience are less likely to leave.

However, if the dataset you received actually contain a column "NumProjects" or "NumberProjects" (some modified versions do), the finding is usually:

>- employees with a very high number of projects (6-7+) have slightly higher attrition (burnout effect),

>- But employees with 2-5 projects have the lowest attrition.

>- Employees with the 1 project actually show higher attriton in some versions.

In the original IBM dataset (without a projects column), the ans is simply:

The is no "number of projects"" variable, and longer tenured employees(who typically have worked on more projects) are less likely to leave.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

sns.set(style="whitegrid")
plt.rcParams['figure.figsize'] = (10,5)

# Set option to explicitly handle future downcasting behavior and suppress warning
pd.set_option('future.no_silent_downcasting', True)

# ------------------------------------------------------------
# 1. Load dataset
# ------------------------------------------------------------
# Using the provided GitHub URL for the dataset
url = "https://raw.githubusercontent.com/MasteriNeuron/datasets/main/hr_analytics.csv"
df = pd.read_csv(url)

print("Columns in dataset:")
print(df.columns.tolist())
print("\nPreview:")
display(df.head())

# ------------------------------------------------------------
# 2. Identify the Attrition Column Automatically
# ------------------------------------------------------------
possible_attr_cols = [c for c in df.columns if
                      "attrition" in c.lower() or
                      "left" in c.lower() or
                      "leave" in c.lower() or
                      "turnover" in c.lower() or
                      "status" in c.lower()]

if not possible_attr_cols:
    raise ValueError("Could not detect attrition column. Tell me your dataset column names.")
attr_col = possible_attr_cols[0]
print("\nDetected attrition column:", attr_col)

# ------------------------------------------------------------
# 3. Create a binary Attrition Flag
# ------------------------------------------------------------
df[attr_col] = df[attr_col].astype(str)

df['Attrition_flag'] = df[attr_col].replace({
    "Yes":1, "No":0,
    "yes":1, "no":0,
    "1":1, "0":0,
    "Resigned":1, "Employed":0,
    "Left":1, "Stayed":0
}).astype(int)

print("\nUnique values in Attrition_flag:", df['Attrition_flag'].unique())


# ------------------------------------------------------------
# 4. Correlation of numerical features with Attrition
# ------------------------------------------------------------
num_cols = df.select_dtypes(include=['number'])
corr = num_cols.corr()['Attrition_flag'].sort_values(ascending=False)

print("\nCorrelation with Attrition:")
print(corr)

plt.figure(figsize=(12,6))
sns.heatmap(num_cols.corr(), cmap="coolwarm", linewidths=0.4)
plt.title("Correlation Heatmap")
plt.show()

# ------------------------------------------------------------
# 5. Key Drivers Visualization
# ------------------------------------------------------------

# Satisfaction (if present)
for col in ["JobSatisfaction", "EnvironmentSatisfaction", "WorkLifeBalance"]:
    if col in df.columns:
        plt.figure()
        sns.boxplot(x=df['Attrition_flag'], y=df[col])
        plt.title(f"{col} vs Attrition")
        plt.xlabel("Attrition (1 = Left)")
        plt.show()

# Overtime
if "OverTime" in df.columns:
    plt.figure()
    sns.countplot(x="OverTime", hue="Attrition_flag", data=df)
    plt.title("OverTime vs Attrition")
    plt.show()

# Salary
for col in ["MonthlyIncome", "Salary", "Income"]:
    if col in df.columns:
        plt.figure()
        sns.boxplot(x=df['Attrition_flag'], y=df[col])
        plt.title(f"{col} vs Attrition")
        plt.show()

# ------------------------------------------------------------
# 6. Are employees with more projects more likely to leave?
# ------------------------------------------------------------
possible_proj_cols = [c for c in df.columns if
                      "project" in c.lower() or
                      "projects" in c.lower() or
                      "num" in c.lower() and "project" in c.lower()]

if possible_proj_cols:
    proj_col = possible_proj_cols[0]
    print("\nDetected project column:", proj_col)

    plt.figure()
    sns.boxplot(x=df['Attrition_flag'], y=df[proj_col])
    plt.title("Do employees with more projects leave more?")
    plt.xlabel("Attrition (1 = Left)")
    plt.show()

    print("\nAverage projects:")
    print(df.groupby("Attrition_flag")[proj_col].mean())
else:
    print("\nNo 'projects' column detected in your dataset.")

# ------------------------------------------------------------
# 7. Summary insights
# ------------------------------------------------------------
print("\n================= SUMMARY ================")
print("1. The above correlations tell you which numerical features drive attrition.")
print("2. The boxplots help identify differences in satisfaction, overtime, salary.")
print("3. The last section shows if more projects = higher\u00a0attrition.")